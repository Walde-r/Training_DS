{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa2a5671",
   "metadata": {},
   "source": [
    "> Ранее мы с вами рассмотрели основы бинарной классификации. Но что делать, когда классов, на которые необходимо разделить данные, больше 2? Например, классификация автомобилей по различным маркам или определение национальности по фотографии и т. д.\n",
    "\n",
    "В таком случае используется очень простой подход, который называется `«один против всех» (one-vs-over)`.\n",
    "\n",
    "Идея этого подхода очень простая. Если у нас есть $k$ различных классов ($k>2$), давайте обучим $k$ классификаторов, каждый из которых будет предсказывать вероятности принадлежности каждого объекта к определённому классу.\n",
    "\n",
    "Например, у нас есть три класса, обозначенные как 0, 1 и 2. Тогда мы обучаем три классификатора: первый из них учится отличать класс 0 от классов 1 и 2, второй — класс 1 от классов 0 и 2, а третий — класс 2 от классов 1 и 0. Таким образом, класс, на который «заточен» классификатор, мы обозначаем как 1, а остальные классы — как 0.\n",
    "\n",
    "Когда каждая из трёх моделей сделает предсказание вероятностей для объекта, итоговый классификатор будет выдавать класс, который соответствует самой «уверенной» модели.\n",
    "\n",
    "Схематично это можно представить следующим образом:\n",
    "\n",
    "<img src='img/ML_3_4_1.png'>\n",
    "\n",
    "Если мы используем в качестве классификатора логистическую регрессию и количество факторов равно двум ($x_1$ и $x_2$), то можно изобразить тепловую карту вероятностей принадлежности к каждому из классов в каждой точке пространства, а также разделяющие плоскости, которые образуются при пороге вероятности в 0.5. \n",
    "\n",
    "<img src='img/dst3-ml3-3_10.png'>\n",
    "\n",
    "На тепловых картах каждый класс обозначен своим цветом: 0 — зелёным, 1 — жёлтым, 2 — синим. Чем ярче цвет, тем выше вероятность принадлежности к каждому к классу в этой области пространства.\n",
    "\n",
    "В результате у нас получится три различных пространства вероятностей, что-то вроде трёх параллельных реальностей. Чтобы собрать всё это воедино, мы выбираем в каждой точке пространства максимум из вероятностей. Получим следующую картину:\n",
    "\n",
    "<img src='img/dst3-ml3-3_11.png'>\n",
    "\n",
    "Модель логистической регрессии легко обобщается на случай мультиклассовой классификации. Пусть мы построили несколько разделяющих плоскостей с различными наборами параметров $k$, где $k$ — номер классификатора. То есть имеем $K$ разделяющих плоскостей:\n",
    "\n",
    "$$\n",
    "z_k = w_{0k} + \\sum_{j=1}^{m} w_{jk}x_j = w_k \\cdot x\n",
    "$$\n",
    "\n",
    "Чтобы преобразовать результат каждой из построенных моделей в вероятности в логистической регрессии, используется функция softmax — многомерный аналог сигмоиды:\n",
    "\n",
    "$$\n",
    "P_k = softmax(z_k) = \\frac{exp(\\hat{y}_k)}{\\sum_{k=1}^{K}exp(\\hat{y}_{jk})}\n",
    "$$\n",
    "\n",
    "Данная функция выдаёт нормированные вероятности, то есть в сумме для всех классов вероятность будет равна 1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myConda_env_312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

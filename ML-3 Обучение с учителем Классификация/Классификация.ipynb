{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59fbe45b",
   "metadata": {},
   "source": [
    "В этом модуле мы продолжаем знакомство с моделями МО в области обучения с учителем. На этот раз поговорим о задаче классификации. Вспомним, где находится классификация на нашей карте машинного обучения:\n",
    "\n",
    "<img src = 'img/dst3-ml1-3_2.jpg'>\n",
    "\n",
    "Вначале мы снова обратимся к классу линейных моделей и рассмотрим `логистическую регрессию`.\n",
    "\n",
    "Затем поговорим о `деревьях решений` для задачи классификации и научимся строить из этих деревьев целый лес.\n",
    "\n",
    "Цели данного модуля:\n",
    "\n",
    "- Познакомиться с принципами работы модели логистической регрессии для решения задачи классификации. \n",
    "- Рассмотреть метрики классификации и научиться оценивать качество моделей, решающих данную задачу.\n",
    "- Узнать принципы построения деревьев решений.\n",
    "- Изучить основы ансамблевых моделей типа бэггинг на примере случайного леса.\n",
    "- Научиться применять деревья решений и случайные леса для решения задачи классификации.\n",
    "\n",
    "Ранее мы обсуждали модель линейной регрессии, которая предназначена для решения задачи регрессии. Теперь нам предстоит разобраться с тем, как преобразовать данную модель, чтобы она решала задачу классификации.\n",
    "\n",
    "Для начала вспомним, что такое `классификация`.\n",
    "\n",
    "Задача `классификации (classification)` — задача, в которой мы пытаемся `предсказать класс объекта` на основе признаков в наборе данных. То есть задача сводится к предсказанию целевого признака, который является категориальным.\n",
    "\n",
    "Когда `классов`, которые мы хотим предсказать, только `два`, классификация называется `бинарной`. Например, мы можем предсказать, болен ли пациент раком, является ли изображение человеческим лицом, является ли письмо спамом и т. д.\n",
    "\n",
    "Когда `классов`, которые мы хотим предсказать, `более двух`, классификация называется `мультиклассовой (многоклассовой)`. Например, предсказание модели самолёта по радиолокационным снимкам, классификация животных на фотографиях, определение языка, на котором говорит пользователь, разделение писем на группы.\n",
    "\n",
    "> Для простоты мы пока разберёмся с бинарной классификацией, а в следующем юните обобщим результат на мультиклассовую\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a95847",
   "metadata": {},
   "source": [
    "Что вообще означает «решить задачу классификации»? Это значит построить разделяющую поверхность в пространстве признаков, которая делит пространство на части, каждая из которых соответствует определённому классу. \n",
    "\n",
    "Ниже представлены примеры разделяющих поверхностей, которые производят бинарную классификацию. Красным и синим цветом обозначены классы, зелёным — собственно поверхность, которая делит пространство признаков на две части. В каждой из этих частей находятся только наблюдения определённого класса.\n",
    "\n",
    "<img src='img/dst3-ml1-3_4.jpg'>\n",
    "\n",
    "> `Модели`, которые `решают задачу классификации`, называются `классификаторами` (classifier).\n",
    "\n",
    "Если взять в качестве разделяющей поверхности некоторую плоскость (ровная поверхность на первом рисунке), то мы получаем модель логистической регрессии, которая тесно связана с рассмотренной нами ранее линейной регрессией.\n",
    "\n",
    "Давайте для начала вспомним, как выглядит уравнение модели линейной регрессии в общем случае:\n",
    "\n",
    "$\\hat{y} = w_0 + w_1x_1 + w_2x_2 + ... + w_mx_m = w_0 + \\sum_{j+1}^{m} w_jx_j$\n",
    "\n",
    "В общем случае это уравнение гиперплоскости, которая стремится приблизить зависимость целевой переменной от $m$ факторов.\n",
    "\n",
    "- Когда фактор всего один, уравнение задаёт прямую:\n",
    "\n",
    "$\\hat{y} = w_0 + w_1x$\n",
    "\n",
    "- Когда факторов два, уравнение задаёт плоскость:\n",
    "\n",
    "$\\hat{y} = w_0 + w_1x_1 + w_2x_2$\n",
    "\n",
    "Но всё это работает `только` `в` том `случае`, когда `целевой признак` , который мы хотим предсказать, является `числовым`, например цена, вес, время аренды и т. д.\n",
    "\n",
    "Что же делать с этой моделью, когда целевой признак $y$ является категориальным? Например, является письмо спамом или обычным письмом?\n",
    "\n",
    "Можно предположить, что, раз у нас есть две категории, мы можем обозначить категории за $y = 1$ (Спам) и $y=0$ (Не спам) и обучить линейную регрессию предсказывать 0 и 1.\n",
    "\n",
    "Но результат будет очень плохим. Выглядеть это будет примерно так:\n",
    "\n",
    "<img src='img/dst3-ml1-3_5.jpg'>\n",
    "\n",
    "Для больших значений $x$ прямая будет выдавать значения больше 1, а для очень маленьких — меньше 0. Что это значит? Непонятно. Непонятно и то, что делать со значениями в диапазоне от 0 до 1. Да, можно относить значения на прямой выше 0.5 к классу 1, а меньше либо равным 0.5 — к классу 0, но это всё «костыли».\n",
    "\n",
    "> Идея! Давайте переведём задачу классификации в задачу регрессии. Вместо предсказания класса будем предсказывать вероятность принадлежности к этому классу. \n",
    "\n",
    "Модель должна выдавать некоторую вероятность $P$, которая будет определять, принадлежит ли данный объект к классу 1: например, вероятность того, что письмо является спамом. При этом вероятность того, что письмо является обычным письмом (класс 0), определяется как $Q = 1 - P$.\n",
    "\n",
    "Когда модель будет обучена на предсказание вероятности, мы зададим некоторый `порог вероятности`. Если предсказанная вероятность будет выше этого порога, мы определим объект к классу 1, а если ниже — к классу 0.\n",
    "\n",
    "Например, стандартный порог равен 0.5. То есть если вероятность $P > 0.5$ , мы будем считать письмо спамом, а если $P \\leq 0.5 $ — обычным информативным письмом.\n",
    "\n",
    "Однако остался главный вопрос: как научить модель предсказывать вероятности, ведь они должны лежать строго в диапазоне от 0 до 1, а предсказания линейной регрессии лежат в диапазоне от $-\\infty$ до $+\\infty$? \n",
    "\n",
    "Тут-то мы и приходим к модели логистической регрессии — `регрессии вероятностей`.\n",
    "\n",
    "#### ОБЩЕЕ ПРЕДСТАВЛЕНИЕ О ЛОГИСТИЧЕСКОЙ РЕГРЕССИИ\n",
    "\n",
    "> `Логистическая регрессия` (Logistic Regression) — одна из простейших моделей для решения задачи классификации. Несмотря на простоту, модель входит в топ часто используемых алгоритмов классификации в `Data Science`.\n",
    "\n",
    "В основе логистической регрессии лежит логистическая функция (logistic function) $\\sigma (z)$ — отсюда и название модели. Однако более распространённое название этой функции — `сигмόида` (sigmoid). Записывается она следующим образом:\n",
    "\n",
    "$\\sigma (z) = \\frac{1}{1+e^{-z}}$\n",
    "\n",
    "> **Примечание**. Здесь $e$ — экспонента или `число Эйлера`. Это число является бесконечным, а его значение обычно принимают равным 2.718... Почитать о нём подробнее вы можете [здесь](https://dzen.ru/a/XKW4kcYE3AId802p).\n",
    "\n",
    "А вот график зависимости сигмоиды от аргумента $z$:\n",
    "\n",
    "<img src='img/dst3-ml1-3_6.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd273cd",
   "metadata": {},
   "source": [
    "> В чём преимущество этой функции?\n",
    "\n",
    "> У сигмоиды есть два очень важных для нас свойства:\n",
    "\n",
    "> Значения сигмоиды $\\sigma (z)$ лежат в диапазоне от 0 до 1 при любых значения аргумента $z$: какой бы $z$ вы ни подставили, число меньше 0 или больше 1 вы не получите.\n",
    "\n",
    "> Сигмоида выдаёт значения $\\sigma (z) > 0.5$ при её аргументе $z > 0$, $\\sigma (z) < 0.5$ — при $z < 0$ и 0.5 — при $z = 0$;.\n",
    "\n",
    "Это ведь и есть свойства вероятности! Выходом сигмоиды является число от 0 до 1, которое можно интерпретировать как вероятность принадлежности к классу 1. Её мы и пытаемся предсказать.\n",
    "\n",
    "Основная идея модели логистической регрессии: возьмём модель линейной регрессии (обозначим её выход за $z$)\n",
    "\n",
    "$z = w_0 + \\sum_{j+1}^{m} w_jx_j$\n",
    "\n",
    "и подставим выход модели $z$ в функцию сигмоиды, чтобы получить искомые оценки вероятности (в математике принято писать оценочные величины с «шапкой» наверху, а истинные значения — без «шапки», это чистая формальность):\n",
    "\n",
    "$\\hat{P} = \\sigma(z) = \\frac{1}{1+e^{-z}} = \\frac{1}{1+e^{-w_0 -\\sum_{j+1}^{m} w_jx_j}} = \\frac{1}{1+e^{\\bar{w}\\cdot\\bar{x}}}$\n",
    "\n",
    "> `Примечание`. Далее в модуле мы будем называть оценки вероятности $\\hat{P}$ просто вероятностью, но только для краткости. Это не значит, что эти оценки являются истинными вероятностями принадлежности к каждому из классов (их нельзя сосчитать, так как для этого нужна выборка бесконечного объёма). Если вы употребляете термин «вероятности» на собеседованиях, обязательно предварительно укажите, что вы подразумеваете `оценку вероятности`.\n",
    "\n",
    "Обучать будем всё в совокупности, пытаясь получить наилучшую оценку вероятности $\\hat{P}$. Если вероятность $\\hat{P} > 0.5$, относим объект к классу 1, а если $\\hat{P} \\leq 0.5$, относим объект к классу 0. \n",
    "\n",
    "Математически это записывается следующей формулой:\n",
    "\n",
    "$$\n",
    "\\hat{y} = I[\\hat{P}] = \\left\\{\n",
    "  \\begin{array}{ll}\n",
    "    1, & \\hat{P} > 0.5 \\\\\n",
    "    0, & \\hat{P} \\leq 0.5\n",
    "  \\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "> `Примечание`. В данном выражении $I[\\hat{P}]$ называется индикаторной функцией. Она возвращает 1, если её значение больше какого-то порога, и 0 — в противном случае. Математики часто записывают просто квадратные скобки, опуская символ $I$: $[\\hat{P}]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f359a40",
   "metadata": {},
   "source": [
    "##### Чего мы добились таким преобразованием?\n",
    "\n",
    "Если мы обучим модель, то есть подберём  коэффициенты $w_0, w_1, w_2, ..., w_m$ (как их найти, обсудим чуть позже) таким образом, что для объектов класса 1 модель линейной регрессии начнёт выдавать положительное число, а для класса 0 — выдавать отрицательное число, то тогда, подставив предсказание линейной регрессии $z$ в сигмоиду, мы сможем получать вероятности принадлежности к каждому из классов в диапазоне от 0 до 1.\n",
    "\n",
    "Далее по порогу вероятности мы сможем определять, к какому классу принадлежит объект.\n",
    "\n",
    "Это и есть наша цель. Мы свели задачу классификации к задаче регрессии для предсказания вероятностей. \n",
    "\n",
    "Для бинарной классификации описанное выше будет выглядеть следующим образом:\n",
    "<img src='img/dst3-ml1-3_7.jpg'>\n",
    "\n",
    "Рассмотрим, как это работает, на примере.\n",
    "\n",
    "Пусть мы каким-то образом обучили модель линейной регрессии предсказывать положительные числа для спам-писем и отрицательные — для обычных писем.\n",
    "\n",
    "Подаём характеристики письма $x_1, x_2, ..., x_m$ в выражение для линейной регрессии и получаем ответ модели, например $z = 1.5$. Тогда, подставив его в сигмоиду, получим:\n",
    "\n",
    "$\\hat{P} = \\sigma (z) = \\frac{1}{1+e^{-1.5}} = 0.82$\n",
    "\n",
    "Таким образом, вероятность того, что данный объект принадлежит классу спама, равна 0.82, что больше порогового значения 0.5. То есть мы относим данное письмо к спаму: $\\hat{y} = 1$ .\n",
    "\n",
    "Пусть теперь мы подали на вход модели характеристики другого письма и получили $z = -0.91$. Тогда, подставив этот результат в сигмоиду, получим:\n",
    "\n",
    "$\\hat{P} = \\sigma (z) = \\frac{1}{1+e^{0.91}} = 0.28$\n",
    "\n",
    "Вероятность того, что данный объект принадлежит классу спама, равна 0.28, что меньше порогового значения 0.5. Мы относим данное письмо к обычным письмам: $\\hat{y} = 0$.\n",
    "\n",
    "Кстати, вероятность того, что это письмо будет обычным, равна противоположной вероятности: $Q = 1 - 0.28 = 0.72$ \n",
    "\n",
    "Полученное выражение для оценки вероятности $\\hat{P}$ и будет называться моделью логистической регрессии:\n",
    "\n",
    "$\\hat{P} = \\frac{1}{1+e^{-w_0-\\sum_{j=1}^{m}w_jx_j}}$\n",
    "\n",
    "$\\hat{y} = I[\\hat{P}]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e0e537",
   "metadata": {},
   "source": [
    "##### Разберёмся с геометрией\n",
    "\n",
    "Возьмём частный случай, когда класс объекта зависит от двух признаков — $x_1$ и $x_2$.\n",
    "\n",
    "Рассмотрим пример.\n",
    "\n",
    "Мы пытаемся предсказать поступление студента в аспирантуру в зависимости от результатов двух экзаменов. Целевой признак  $y$ — результат поступления (`admission outcome`) с двумя возможными значениями: `поступил` или `не поступил`. Факторы: $x_1$ — результат сдачи первого экзамена (Exam1 Score) и $x_2$ — результат сдачи второго (Exam 2 Score). Будем предсказывать вероятность поступления с помощью логистической регрессии.\n",
    "\n",
    "Изобразим зависимость в пространстве двух факторов (вид сверху) в виде диаграммы рассеяния, а целевой признак отобразим в виде точек (непоступившие) и крестиков (поступившие).\n",
    "\n",
    "Если рассматривать уравнение линейной регрессии отдельно от сигмоиды, то геометрически построить логистическую регрессию на основе двух факторов — значит найти такие коэффициенты $w_0$, $w_1$ и $w_2$ уравнения плоскости, при которых наблюдается наилучшее разделение пространства на две части.\n",
    "\n",
    "$z = w_0 + w_1x_1 + w_2x_2$\n",
    "\n",
    "Тогда выражение для $z$ будет задавать в таком пространстве плоскость (в проекции вида сверху — прямую), которая разделяет всё пространство на две части. Над прямой вероятность поступления будет > 0.5, а под прямой < 0.5>:\n",
    "\n",
    "<img src='img/dst3-ml1-3_8.jpg'>\n",
    "\n",
    "<div style=\"border: 1px solid yellow; padding: 10px;\">\n",
    "<table>\n",
    "    <td>Кулинарная аналогия\n",
    "\n",
    "Возьмите по пригоршне риса и гречки и рассыпьте крупы на столе. Попытайтесь наложить лист бумаги вертикально на плоскость стола так, чтобы максимально качественно отделить виды круп друг от друга.\n",
    "\n",
    "Лист бумаги и будет разделяющей плоскостью. Вам необходимо найти такое расположение листа, при котором разделение будет наилучшим.</td>\n",
    "    <td><img src='img/dst3-ml3-2_6.png'></td>\n",
    "</table>\n",
    "</div>\n",
    "\n",
    "Коэффициенты построенной выше плоскости равны (как их найти, обсудим позже):\n",
    "\n",
    "$$\n",
    "    w_0 = -25.05 \\\\\n",
    "    w_1 = 0.205 \\\\\n",
    "    w_2 = 0.2\n",
    "$$\n",
    "\n",
    "Тогда модель логистической регрессии будет иметь вид:\n",
    "\n",
    "$$\n",
    "    z = -25.05 + 0.205x_1 + 0.2x_2 \\\\\n",
    "\n",
    "    \\hat{P} = \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "Появляется новый абитуриент, и мы хотим предсказать вероятность его поступления. Баллы студента: $x_1 = 67$, $x_2 = 53$. Заметьте, что точка с такими координатами находится ниже нашей плоскости (то есть абитуриент, скорее всего, не поступит).\n",
    "\n",
    "Тогда:\n",
    "\n",
    "$$\n",
    "    z = -25.05 + 0.205 \\cdot 67 +0.2 \\cdot 53 = -0.71\\\\\n",
    "    \\hat{P} = \\sigma(z) = \\frac{1}{1 + e^{0.71}} = 0.32\n",
    "$$\n",
    "\n",
    "Итак, оценка вероятности поступления студента составляет 0.32, то есть его шанс поступления составляет 32%.\n",
    "\n",
    "А что если мы возьмём точку, лежащую выше прямой?\n",
    "\n",
    "Например, появился абитуриент с баллами $x_1 = 80$, $x_2 = 75$. Подставим его баллы в нашу модель логистической регрессии, чтобы понять, какова оценочная вероятность поступления:\n",
    "\n",
    "$$\n",
    "    z = -25.05 + 0.205 \\cdot 80 +0.2 \\cdot 75 = 6.34\\\\\n",
    "    \\hat{P} = \\sigma(z) = \\frac{1}{1 + e^{-6.34}} = 0.99\n",
    "$$\n",
    "\n",
    "Таким образом, оценка вероятности поступления абитуриента составляет 0.99, шанс поступления — 99 %.\n",
    "\n",
    "В чём математический секрет?\n",
    "\n",
    "Математически подстановка в уравнение плоскости точки, которая не принадлежит ей (находится ниже или выше), означает вычисление расстояния от этой точки до плоскости.\n",
    "\n",
    "- Если точка находится ниже плоскости, расстояние будет отрицательным ($z<0$).\n",
    "- Если точка находится выше плоскости, расстояние будет положительным ($z>0$).\n",
    "- Если точка находится на самой плоскости, $z = 0$ .\n",
    "\n",
    "Мы знаем, что подстановка отрицательных чисел в сигмоиду приведёт к вероятности $\\hat{P} = 0.5$, а постановка положительных — к вероятности $\\hat{P} > 0.5$ . \n",
    "\n",
    "<div style=\"border: 1px solid yellow; padding: 10px;\">\n",
    "    Таким образом, ключевым моментом в предсказании логистической регрессии является расстояние от точки до разделяющей плоскости в пространстве факторов. Это расстояние в литературе часто называется **отступом (margin)**. \n",
    "</div>\n",
    "\n",
    "В этом и состоит секрет работы логистической регрессии.\n",
    "\n",
    "> Чем больше расстояние от точки, находящейся выше разделяющей плоскости, до самой плоскости, тем больше оценка вероятности принадлежности к классу 1\n",
    "\n",
    "Попробуйте подставить различные координаты точек в модель логистической регрессии и убедитесь в этом.\n",
    "\n",
    "Можно построить тепловую карту, которая показывает, чему равны вероятности в каждой точке пространства:\n",
    "\n",
    "<img src='img/dst3-ml1-3_9.jpg'>\n",
    "\n",
    "На рисунке точки, которые относятся к классу непоступивших абитуриентов, лежащие ниже разделяющей плоскости, находятся в красной зоне. Чем насыщеннее красный цвет, тем ниже вероятность того, что абитуриент поступит в аспирантуру. И наоборот, точки, которые относятся к классу поступивших абитуриентов, лежащие выше разделяющей плоскости, находятся в синей зоне. Чем насыщеннее синий цвет, тем выше вероятность того, что абитуриент поступит в аспирантуру.\n",
    "\n",
    "Для случая зависимости целевого признака $y$ от трёх факторов $x_1, x_2\\text{ и }x_3$, например от баллов за два экзамена и рейтинга университета, из которого выпустился абитуриент, выражение для $z$ будет иметь вид:\n",
    "\n",
    "$z = w_0 + w_1x_1 + w_2x_2 + w_3x_3$\n",
    "\n",
    "Уравнение задаёт плоскость в четырёхмерном пространстве. Но если вспомнить, что $y$ — категориальный признак и классы можно обозначить цветом, то получится перейти в трёхмерное пространство. Разделяющая плоскость будет выглядеть следующим образом:\n",
    "\n",
    "<img src='img/dst3-ml1-3_10.jpg'>\n",
    "\n",
    "В общем случае, когда у нас есть зависимость от  факторов, линейное выражение, находящееся под сигмоидой, будет обозначать разделяющую **гиперплоскость**.\n",
    "\n",
    "$z = w_0 + w_1x_1 + w_2x_2 + ... + w_mx_m = w_0 + \\sum_{j=1}^{m} w_jx_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e33020",
   "metadata": {},
   "source": [
    "#### ПОИСК ПАРАМЕТРОВ ЛОГИСТИЧЕСКОЙ РЕГРЕССИИ\n",
    "\n",
    "Итак, мы разобрались с тем, как выглядит модель логистической регрессии и что она означает в геометрическом смысле.\n",
    "\n",
    "> Но остался один главный вопрос: как найти такие коэффициенты $w = (w_1, w_2,..., w_m)$, чтобы гиперплоскость разделяла пространство наилучшим образом? \n",
    "\n",
    "Вновь обратимся к нашей схеме минимизации эмпирического риска:\n",
    "\n",
    "<img src='img/dst3-ml1-3_11.jpg'>\n",
    "\n",
    "Можно предположить, что стоит использовать метод наименьших квадратов. Введём функцию ошибки — средний квадрат разности `MSE` между истинными классами  и предсказанными классами  и попытаемся его минимизировать.\n",
    "\n",
    "Сразу можно достоверно предсказать, что результат такого решения будет плохим, поэтому воздержимся от его использования.\n",
    "\n",
    "Здесь нужен другой подход. Это метод `максимального правдоподобия` (Maximum Likelihood Estimation — `MLE`). \n",
    "\n",
    "> Правдоподобие — это оценка того, насколько вероятно получить истинное значение целевой переменной $y$ при данных $x$ и параметрах $w$.\n",
    "\n",
    "Данный метод позволяет получить функцию правдоподобия.\n",
    "\n",
    "Цель метода — найти такие параметры $w = (w_1, w_2,..., w_m)$, в которых наблюдается максимум функции правдоподобия. Подробнее о выводе формулы вы можете прочитать [здесь](https://habr.com/ru/post/485872/).\n",
    "\n",
    "А мы пока что опустим математические детали метода и приведём только конечную формулу:\n",
    "\n",
    "$likilihood = \\sum_i^n(y_i log(\\hat{P}_i)+(1-y_i) log(1 - \\hat{P}_i)) \\rightarrow max_w $\n",
    "\n",
    "Не пугайтесь. Давайте разберёмся, что есть что и как работает эта функция.\n",
    "\n",
    "- $n$ — количество наблюдений.\n",
    "- $y_i$ — это истинный класс (1 или 0) для $i$-ого объекта из набора данных.\n",
    "- $\\hat{P}_i = \\sigma(z_i)$ — предсказанная с помощью логистической регрессии вероятность принадлежности к классу 1 для $i$-ого объекта из набора данных.\n",
    "$z_i$ — результат подстановки $i$-ого объекта из набора данных в уравнение разделяющей плоскости $z_i = \\bar{w} \\cdot \\bar{x}_i$.\n",
    "-$\\log$ — логарифм (обычно используется натуральный логарифм по основанию $ e - ln$).\n",
    "\n",
    "<div style=\"border: 1px solid yellow; padding: 10px;\">\n",
    "<b>Пример расчёта функции правдоподобия</b>\n",
    "\n",
    "Вернёмся к примеру с абитуриентами. Пусть у нас есть выборка из четырёх студентов с оценками по двум экзаменам:  и . Возьмём уравнение разделяющей плоскости, которое мы использовали ранее:\n",
    "$$\n",
    "    z = -25.05 + 0.205x_1 + 0.2x_2\n",
    "$$\n",
    "\n",
    "Мы взяли всех студентов из выборки в формулу сигмоиды и получили оценочную вероятность поступления каждого из студентов:\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "    <td>i</td>\n",
    "    <td>1</td>\n",
    "    <td>2</td>\n",
    "    <td>3</td>\n",
    "    <td>4</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>$\\hat{P}_i$ — оценка вероятности </td>\n",
    "    <td>0.2</td>\n",
    "    <td>0.8</td>\n",
    "    <td>1</td>\n",
    "    <td>0.6</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>$y_i$ - истинный класс</td>\n",
    "    <td>0</td>\n",
    "    <td>0</td>\n",
    "    <td>1</td>\n",
    "    <td>1</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Подсчитаем, чему равна функция правдоподобия при данных предсказаниях вероятностей:\n",
    "\n",
    "$$\n",
    "likilihood = \\sum_i^n(y_i log(\\hat{P}_i)+(1-y_i) log(1 - \\hat{P}_i)) = \\\\\n",
    "((0log(0.2)+(1-0)log(1-0.2))+\\\\\n",
    "(0log(0.8)+(1+0)log(1-0.8))+\\\\\n",
    "(1log(1)+(1-1)log(1-1))+\\\\\n",
    "(1log(0.6)+(1-1)log(1-0.6)))=\\\\\n",
    "(log(0.8)+log(0.2)+log(1)+log(0.6)) = -2.34 \n",
    "$$\n",
    "</div>\n",
    "\n",
    "Такие расчёты можно производить для любых значений параметров, меняется только оценка вероятности $\\hat{P}_i$.\n",
    "\n",
    "> **Примечание**. К сожалению, функция `likelihood` не имеет интерпретации, то есть нельзя сказать, что значит число -2.34 в контексте правдоподобия.\n",
    "\n",
    "Цель — найти такие параметры, при которых наблюдается максимум этой функции.\n",
    "\n",
    "Теперь пора снова применить магию математики, чтобы привести задачу к привычному нам формату минимизации эмпирического риска. По правилам оптимизации, если поставить перед функцией минус, то задача оптимизации меняется на противоположную: был поиск максимума — станет поиском минимума.\n",
    "\n",
    "Таким образом мы получим функцию потерь $L(w)$, которая носит название `«функция логистических потерь»`, или `logloss`. Также часто можно встретить название `кросс-энтропия`, или `cross-entropy loss`:\n",
    "\n",
    "$$L(w) = logloss = - \\sum_i^n(y_i log(\\hat{P}_i)+(1-y_i) log(1 - \\hat{P}_i)) \\rightarrow min_w \\\\\n",
    "\\hat{P}_i = \\frac{1}{1+e^{-w_0-\\sum_{j-1}^mw_jx_j}}\n",
    "$$\n",
    "\n",
    "Вот эту функцию мы и будем минимизировать в рамках поиска параметров логистической регрессии. Мы должны найти такие параметры разделяющей плоскости $w$, при которых наблюдается минимум `logloss`.\n",
    "\n",
    "Знакомая задача? Всё то же самое, что и с линейной регрессией, только функция ошибки другая.\n",
    "\n",
    "> К сожалению, для такой функции потерь аналитическое решение оптимизационной задачи найти не получится: при расчётах получается, что его попросту не существует.\n",
    "\n",
    "Но мы помним, что, помимо аналитических решений, есть и численные.\n",
    "\n",
    "Например, для поиска параметров можно использовать знакомый нам градиентный спуск. Вспомним, как выглядит итерационная формула данного метода:\n",
    "\n",
    "$$\n",
    "w^{k+1} = w^{(k)} - \\eta \\nabla L \\left( \\omega^{(k)} \\right)\n",
    "$$\n",
    "\n",
    "Повторим её смысл: \n",
    "новое значение параметров $w^{k+1}$ получается путём сдвига текущих $w^{(k)}$ в сторону вектора антиградиента $\\nabla L \\left( \\omega^{(k)} \\right)$, умноженного на темп обучения $\\eta$.\n",
    "\n",
    "Математическую реализацию вычисления градиента для `logloss` мы обсудим далее в курсе, а пока нас интересует исключительно его смысл.\n",
    "\n",
    "> Мы уже знаем, что для того, чтобы повысить шанс пройти мимо локальных минимумов функции потерь, используется не сам градиентный спуск, а его модификации: например, можно использовать уже знакомый нам стохастический градиентный спуск (`SGD`).\n",
    "\n",
    "> Помним, что применение градиентного спуска требует предварительного масштабирования данных (стандартизации/нормализации). В реализации логистической регрессии в `sklearn` предусмотрено ещё несколько методов оптимизации, для которых масштабирование не обязательно. О них мы упомянем в практической части модуля.\n",
    "\n",
    "Во избежание переобучения модели в функцию потерь логистической регрессии традиционно добавляется регуляризация. В реализации логистической регрессии в `sklearn` она немного отличается от той, что мы видели ранее для линейной регрессии.\n",
    "\n",
    "При L1-регуляризации мы добавляем в функцию потерь $L(w)$ штраф из суммы модулей параметров, а саму функцию `logloss` умножаем на коэффициент $C$:\n",
    "\n",
    "$$\n",
    "L(w) = C \\cdot logloss + \\sum_{j=1}^{m}|w_j| \\rightarrow min_w\n",
    "$$\n",
    "\n",
    "А при L2-регуляризации — штраф из суммы квадратов параметров:\n",
    "\n",
    "$$\n",
    "L(w) = C \\cdot logloss + \\sum_{j=1}^{m}(w_j)^2 \\rightarrow min_w\n",
    "$$\n",
    "\n",
    "Значение коэффициента $C$ — коэффициент, обратный коэффициенту регуляризации. Чем **больше** $C$, тем **меньше** «сила» регуляризации.\n",
    "\n",
    "Предлагаем вам посмотреть на то, как будет меняться форма сигмоиды, разделяющей плоскости при минимизации функции потерь logloss (она обозначена как cross-entropy в виде концентрических кругов — вид сверху), с помощью обычного градиентного спуска (не стохастического) в виде анимации.\n",
    "\n",
    "\n",
    "<video src='img/dst3-ml1-3_11.mp4' controls autoplay loop>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41690ad",
   "metadata": {},
   "source": [
    "[Источник](https://medium.com/swlh/from-animation-to-intuition-linear-regression-and-logistic-regression-f641a31e1caf) \n",
    "\n",
    "Не волнуйтесь, все громоздкие формулы уже реализованы в классических библиотеках, таких как `sklearn`. Но нам важно понимать принцип того, что происходит «под капотом», чтобы верно интерпретировать результат и по возможности управлять им.\n",
    "\n",
    "далее смотри [ноутбук](ML-3._Логистическая_регрессия.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myConda_env_312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

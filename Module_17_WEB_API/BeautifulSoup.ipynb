{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63a072a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # Импортируем библиотеку requests\n",
    "url = 'https://nplus1.ru/news/2021/10/11/econobel2021' # Определяем адрес страницы\n",
    "response = requests.get(url)  # Выполняем GET-запрос\n",
    "#print(response.text)  # Выводим содержимое атрибута text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a1421d",
   "metadata": {},
   "source": [
    "✍ Для поиска необходимых нам данных мы будем использовать библиотеку `BeautifulSoup`, которая позволяет по названию тегов и их атрибутов получать содержащийся в них текст.\n",
    "\n",
    "`BeautifulSoup` не является частью стандартной библиотеки, поэтому для начала её нужно установить. Например, в Jupyter Notebook это делается с помощью такой команды:\n",
    "\n",
    "```python\n",
    "# Устанавливаем библиотеку BeautifulSoup\n",
    "!pip install beautifulsoup4 \n",
    "После установки импортируем библиотеку в наш код:\n",
    "```\n",
    "\n",
    "После установки импортируем библиотеку в наш код:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6abc37d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # Импортируем библиотеку BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06c0a34",
   "metadata": {},
   "source": [
    "Теперь мы можем извлекать данные из любой веб-страницы.\n",
    "\n",
    "Ранее мы уже получили содержимое страницы с помощью `GET-запроса` и сохранили информацию в переменной `response` , теперь создадим объект `BeautifulSoup` с именем `page`, указывая в качестве параметра `html.parser`.\n",
    "\n",
    "Для примера получим информацию o `title` (с англ. заголовок) — это строка, которая отображается на вкладке браузера:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe74e2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Премию Нобеля по экономике присудили за исследования экономики труда и причинно-следственных связей</title>\n",
      "Премию Нобеля по экономике присудили за исследования экономики труда и причинно-следственных связей\n"
     ]
    }
   ],
   "source": [
    "#import requests # Импортируем библиотеку requests\n",
    "#from bs4 import BeautifulSoup # Импортируем библиотеку BeautifulSoup\n",
    "#url = 'https://nplus1.ru/news/2021/10/11/econobel2021' # Определяем адрес страницы\n",
    "#response = requests.get(url) # Выполняем GET-запрос, содержимое ответа присваивается переменной response\n",
    "page = BeautifulSoup(response.text, 'html.parser') # Создаём объект BeautifulSoup, указывая html-парсер\n",
    "print(page.title) # Получаем тег title, отображающийся на вкладке браузера\n",
    "print(page.title.text) # Выводим текст из полученного тега, который содержится в атрибуте text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d802d6a",
   "metadata": {},
   "source": [
    "Если при запросе к сайту, а затем при его разборе с помощью `BeautifulSoup` в тексте страницы не находится нужный тег, попробуйте вывести на печать пару тысяч символов текста страницы. \n",
    "\n",
    "сли там обнаружится нечто похожее на капчу, возможно, сайт посчитал вас роботом и отказывается выдавать содержимое. Чтобы получить его, попробуйте «притвориться» браузером при запросе из скрипта:\n",
    "\n",
    "> requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "\n",
    "`User-Agent` своего браузера можно узнать по этой [ссылке](https://whatmyuseragent.com/).\n",
    "\n",
    "### Извлекаем заголовок статьи\n",
    "\n",
    "Выполним поставленную ранее задачу: получить информацию о странице и извлечь заголовок статьи, опубликованной на этой странице, дату публикации, а также текст статьи.\n",
    "\n",
    "```HTML\n",
    "Предположим, что мы знаем, что в HTML-коде рассматриваемой нами страницы заголовок статьи заключён в тег <h1> … </h1> (заголовок первого уровня).\n",
    "```\n",
    "\n",
    "Тогда мы можем получить его текст с помощью метода `find()` (с англ. найти) объекта `BeautifulSoup`, передав ему название интересующего нас тега:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4de42a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            Премию Нобеля по экономике присудили за исследования экономики труда и причинно-следственных связей\n",
      "          \n"
     ]
    }
   ],
   "source": [
    "# Применяем метод find() к объекту и выводим результат на экран\n",
    "print(page.find('h1').text) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7053b628",
   "metadata": {},
   "source": [
    "Но как же узнать, в каких именно тегах заключена необходимая информация?\n",
    "\n",
    "Проще всего это сделать с помощью так называемого инструмента разработчика, который есть во всех современных браузерах. Покажем, как открыть данный инструмент на примере использования браузера Google Chrome.\n",
    "\n",
    "Устанавливаем курсор на элементе страницы (заголовок статьи), информацию о котором хотим получить, нажимаем на правую клавишу мыши и в выпадающем списке выбираем пункт «Просмотреть код элемента» или «Исследовать» в зависимости от браузера.\n",
    "\n",
    "<center> <img src='img/Python_17-extra-02.png' width='60%' height = '60%'> </center>\n",
    "\n",
    "```HTML\n",
    "В открывшемся окне инструмента разработчика видим, что информация о заголовке статьи заключена в теге <h1> … </h1>.\n",
    "```\n",
    "<center> <img src='img/Python_17-extra-03.png' width='60%' height = '60%'> </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d8862a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Operating system'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Напишите функцию wiki_header, которая по адресу страницы возвращает заголовок первого уровня для статей на Wikipedia.\n",
    "#Функция wiki_header принимает один аргумент - url.\n",
    "\n",
    "def wiki_header(url):\n",
    "    import requests \n",
    "    from bs4 import BeautifulSoup\n",
    "    response = requests.get(url)\n",
    "    page = BeautifulSoup(response.text, 'html.parser') \n",
    "    return page.find('h1').text\n",
    "\n",
    "wiki_header('https://en.wikipedia.org/wiki/Operating_system')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d1b7a7",
   "metadata": {},
   "source": [
    "#### Неуникальные теги: извлекаем текст и дату публикации статьи\n",
    "\n",
    "Теперь получим сам текст статьи. Как вы уже знаете, первым делом необходимо определить, в какой тег он заключён. Применим, как и ранее, инструмент разработчика.\n",
    "\n",
    "<center> <img src='img/Python_17_12.png' width='80%' height = '80%'> </center>\n",
    "\n",
    "```HTML\n",
    "Видим, что искомый текст заключён в тег  <div> … </div> . Попробуем извлечь его уже известным нам способом — с помощью метода `find()` — и выведем его на экран.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79960039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(page.find('div').text) # Выводим содержимое атрибута text тега div"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bbfc62",
   "metadata": {},
   "source": [
    "Мы увидели не то, что ожидали — кучу текста, не имеющего отношения к тому, что мы искали...\n",
    "\n",
    "?В чём же проблема?\n",
    "\n",
    "```HTML\n",
    "Дело в том, что теги <div> … </div> очень распространённые и на странице их очень много. Метод `find()` нашёл первый из них, но это не то, что нам надо.\n",
    "```\n",
    "\n",
    "Посмотрим на нашу страницу, используя инструмент разработчика, ещё раз. Можем заметить, что у искомого текста есть свой класс — `n1_material text-18`:\n",
    "\n",
    "<center> <img src='img/Python_17_14.png' width='80%' height = '80%'> </center>\n",
    "\n",
    "Передадим название класса в метод find() с помощью аргумента class_ и получим текст статьи:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e297d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Премия Шведского национального банка по экономическим наукам памяти Альфреда Нобеля за 2021 год присуждена Дэвиду Карду (David Card) за его вклад в эмпирические исследования экономики рынка труда, а также Джошуа Энгристу (Joshua Angrist) и Гвидо Имбенсу (Guido Imbens) за их вклад в методологию анализа причинно-следственных связей. Прямая трансляция церемонии объявления лауреатов шла на официальном сайте Нобелевской премии.\n"
     ]
    }
   ],
   "source": [
    "print(page.find('div', class_='n1_material text-18').text) # Выводим содержимое атрибута text тега div класса n1_material text-18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f47d116",
   "metadata": {},
   "source": [
    "В данном случае происходит поиск точного строкового значения `class` атрибута, т. е. выполнение строк кода:\n",
    "\n",
    "```python\n",
    "> print(page.find('div', class_='n1_material').text)\n",
    "> print(page.find('div', class_='n1_material text-18').text)\n",
    "\n",
    "даст одинаковый результат.\n",
    "При выполнении строки кода\n",
    "\n",
    "> print(page.find('div', class_='text-18 n1_material').text)\n",
    "```\n",
    "мы получим ошибку, так как такого строкового значения в области поиска нет.\n",
    "\n",
    "Аналогично получим информации о теге, который содержит дату написания статьи, отображаемую в левом верхнем углу страницы.\n",
    "\n",
    "<center> <img src='img/Python_17-extra-04.png' width='80%' height = '80%'> </center>\n",
    "\n",
    "```HTML\n",
    "Итак, нам нужен тег <a> … </a> с классом \"relative before:block before:w-px before:bg-current before:h-4 before:absolute before:left-0 group pl-2 flex inline-flex items-center\". \n",
    "```\n",
    "Для поиска достаточно указать в качестве класса \"`relative`\", отбросив дополнительные настройки.\n",
    "\n",
    "Теперь получим данные из него с помощью уже известного метода `find()`, передав название нужного тега:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0acfece4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "11.10.21\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Выводим на экран содержимое атрибута text тега a с классом \"relative\"\n",
    "print(page.find('a', class_= \"relative\").text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d90e868",
   "metadata": {},
   "source": [
    "> О поиске по классу можно узнать подробнее в Beautiful Soup [Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#searching-by-css-class).\n",
    "\n",
    "Задача решена — мы извлекли из контента страницы заголовок статьи, опубликованной на странице, дату публикации, а также текст статьи.\n",
    "\n",
    "#### Сбор нескольких элементов: собираем все ссылки на странице\n",
    "\n",
    "Рассмотрим ещё один сценарий: вы хотите собрать сразу несколько элементов со страницы. Например, представьте, что вы хотите получить названия всех языков программирования, упомянутых на странице в Wikipedia в статье про языки программирования.\n",
    "\n",
    "Можно заметить, что все названия языков программирования на этой странице связаны ссылками c соответствующими статьями о них. Таким образом, нам необходимо собрать все ссылки на странице. \n",
    "```HTML\n",
    "Для ссылок в HTML предусмотрен тег <a> … </a>. Попробуем использовать `find()`:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73e9a4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a class=\"mw-jump-link\" href=\"#bodyContent\">Jump to content</a>\n"
     ]
    }
   ],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/List_of_programming_languages' # Задаём адрес ресурса\n",
    "response = requests.get(url) # Делаем GET-запрос к ресурсу\n",
    "page = BeautifulSoup(response.text, 'html.parser') # Создаём объект BeautifulSoup\n",
    "print(page.find('a')) # Ищем ссылку по тегу <a> и выводим её на экран\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fff5a7",
   "metadata": {},
   "source": [
    "Мы получили только одну ссылку, хотя на странице их явно больше.\n",
    "\n",
    "Это происходит, потому что метод `find()` возвращает только первый подходящий элемент. Если требуется получить больше элементов, необходимо воспользоваться методом `find_all()` (с англ. найти все):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d06c70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "964\n"
     ]
    }
   ],
   "source": [
    "links = page.find_all('a') # Ищем все ссылки на странице и сохраняем в переменной links в виде списка\n",
    "print(len(links)) # Выводим количество найденных ссылок"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248783d1",
   "metadata": {},
   "source": [
    "Итак, на момент создания этого конспекта учебных материалов на странице содержалось 964 ссылок. Посмотрим на некоторые из них:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb0eca05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Magik', 'Magma', 'Maple', 'MAPPER', 'MARK-IV', 'Mary', 'MATLAB', 'MASM Microsoft Assembly x86', 'MATH-MATIC', 'Maude system']\n"
     ]
    }
   ],
   "source": [
    "print([link.text for link in links[500:510]]) # Выводим ссылки с 500 по 509 включительно"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525d7134",
   "metadata": {},
   "source": [
    "Не все ссылки соответствуют названиям языков программирования — страница содержит также «служебные» ссылки, такие, например, как Jump to navigation (с англ. Перейти к навигации) или Alphabetical (с англ. По алфавиту):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72950685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jump to content', 'Main page', 'Contents', 'Current events', 'Random article', 'About Wikipedia', 'Contact us', 'Help', 'Learn to edit', 'Community portal']\n"
     ]
    }
   ],
   "source": [
    "print([link.text for link in links[0:10]]) # Выводим ссылки с 1 по 9 включительно"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f47009c",
   "metadata": {},
   "source": [
    "Для обработки полученных данных и исключения «лишней» информации можно, например, использовать подходы, которые вы изучили в модуле [PYTHON-14](https://lms.skillfactory.ru/courses/course-v1:SkillFactory+DST-3.0+28FEB2021/jump_to_id/c988abcb7a6d4ebeb22cf03b00e8118e).\n",
    "\n",
    "✍ В заключение заметим, что `BeautifulSoup` — достаточно мощная библиотека. Мы рассмотрели её базовые возможности, но их полный список гораздо шире. С ним можно ознакомиться в официальной [документации](https://www.crummy.com/software/BeautifulSoup/bs4/doc.ru/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myConda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

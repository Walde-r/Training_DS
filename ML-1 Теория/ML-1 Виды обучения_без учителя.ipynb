{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90c93321",
   "metadata": {},
   "source": [
    "Продолжаем наше знакомство с видами машинного обучения. На очереди `обучение без учителя`.\n",
    "\n",
    "Вспомним нашу карту машинного обучения, а точнее — её часть, описывающую обучение без учителя:\n",
    "\n",
    "<img src='img/dst3-ml1-4_1.jpg'>\n",
    "\n",
    "### ОБУЧЕНИЕ БЕЗ УЧИТЕЛЯ\n",
    "\n",
    "Разметка данных — роскошь. Например, если мы хотим написать классификатор автобусов, неужели нам придётся идти на улицу и фотографировать миллион автобусов и подписывать, где какой? Согласитесь, не очень приятное занятие: мы учимся на курсе Data Science не для того, чтобы размечать картинки в редакторе вручную.\n",
    "\n",
    "Когда нет разметки, конечно, есть надежда на фрилансеров, которые готовы сделать вашу работу за вас, но это не бесплатно. На самом деле так обычно и поступают на практике. \n",
    "\n",
    "Однако можно попробовать применить `обучение без учителя`. Обучение без учителя подразумевает, что у вас `нет правильных ответов`. То есть признак, который вы хотите предсказать, вам недоступен. Подход основан на том, что алгоритм самостоятельно выявляет зависимости в данных только на основе схожести объектов в данных между собой.\n",
    "\n",
    "Легко понять, что такой подход обладает гораздо меньшей точностью по сравнению с обучением с учителем. Поэтому для решения тех задач, которые мы рассматривали выше, он используется гораздо реже.\n",
    "\n",
    "Обучение без учителя все же чаще используют как метод анализа и предобработки данных.\n",
    "\n",
    "Данный вид машинного обучения разбивается на несколько самостоятельных типов задач:\n",
    "\n",
    "- кластеризация,\n",
    "- понижение размерности,\n",
    "- ассоциация.\n",
    "\n",
    "Давайте подробнее рассмотрим каждую из задач.\n",
    "\n",
    "#### Кластеризация\n",
    "\n",
    "<img src='img/dst3-ml1-4_2.jpg'>\n",
    "\n",
    "Задача `кластеризации` (clustering) — это задача, в которой мы разделяем данные на группы на основе признаков в данных.\n",
    "\n",
    "<img src='img/dst3-ml1-4_3.jpg'>\n",
    "\n",
    "На первый взгляд может показаться, что эта задача идентична классификации, но в задаче кластеризации у нас нет правильных ответов. То есть у нас нет учителя: мы пытаемся разделить данные на категории, опираясь только на «близость» объектов. Причём результаты каждого из алгоритмов кластеризации могут быть разными, потому что «близость» — понятие относительное и её можно измерять различными способами.\n",
    "\n",
    "Примеры использования кластеризации: сегментация рынка на категории, объединение близких точек на карте, разделение клиентов по уровню платёжеспособности, кластеризация студентов по их интересам или обучаемости, анализ и разметка новых данных.\n",
    "\n",
    "Ещё один отличный пример — маркеры на Google Картах. Когда вы ищете все крафтовые бары в Москве, приложение группирует их в кружочки с цифрой — при попытке нарисовать миллион маркеров приложение просто зависло бы. Критерием объединения маркеров является расстояние между ними.\n",
    "\n",
    "Более сложные примеры — приложения iPhoto или Google Фото, которые находят лица людей на фотографиях и группируют их в альбомы. Приложение не знает, как зовут ваших друзей, но может отличить их по характерным чертам лица. Это типичная кластеризация.\n",
    "\n",
    "> **Цель обучения** — построить модель, которая наилучшим образом объединит «похожие» объекты в группы.\n",
    "\n",
    "Количество кластеров в зависимости от выбранного алгоритма можно задать заранее или доверить их количество алгоритму.\n",
    "\n",
    "На рисунке ниже представлен пример кластеризации товаров по двум признакам: цена товара и его популярность. Количество кластеров, которое нашёл алгоритм, — 5.\n",
    "\n",
    "<img src='img/dst3-ml1-4_4.jpg'>\n",
    "\n",
    "`Самая сложная часть` в задаче кластеризации — `проинтерпретировать` полученные `кластеры`, понять, что именно объединяет объекты, попавшие в тот или иной кластер.\n",
    "\n",
    "Типичные методы кластеризации при известном заранее количестве кластеров:\n",
    "\n",
    "- метод k-средних (k-means),\n",
    "- EM-алгоритм,\n",
    "- агломеративная кластеризация.\n",
    "\n",
    "Иногда кластеры могут быть очень сложными и вы можете не знать их количество.\n",
    "\n",
    "> Представьте, что вы — геолог, которому нужно найти на карте схожие по структуре горные породы: ваши кластеры не только будут вложены друг в друга, но вы также не знаете, сколько их вообще получится.\n",
    "\n",
    "Для таких задач используются хитрые методы, например алгоритм DBSCAN. Он сам находит скопления точек и строит вокруг кластеры. Очень рекомендуем посмотреть, как происходит процесс кластеризации алгоритмом DBSCAN в [динамике](https://www.youtube.com/watch?v=krpNHsM267A&t=13s). Это захватывающее зрелище.\n",
    "\n",
    "Математическая постановка задачи кластеризации будет выглядеть следующим образом:\n",
    "\n",
    "Пусть $X$ — множество объектов, $Y$ — множество меток кластеров. Задана функция расстояния между двумя объектами $p(x_i,x_j)$, имеется конечная обучающая выборка объектов $X^m = \\{x_1, x_2, ..., x_m\\} \\subset X$.\n",
    "\n",
    "Требуется разбить выборку на непересекающиеся подмножества (кластеры) так, чтобы каждый кластер состоял из объектов, близких по метрике $p(x_i, x_j)$ а объекты разных классов существенно отличались.\n",
    "\n",
    "Разбиение производит некоторый алгоритм (модель) $a$, который каждому объекту ставит в соответствие кластер $y_i$ из множества $Y$, то есть $a: X \\rightarrow Y$.\n",
    "\n",
    "#### Понижение размерности (обобщение)\n",
    "\n",
    "<img src='img/dst3-ml1-4_5.jpg'>\n",
    "\n",
    "`Понижение размерности` (dimensionality reduction) — задача, в которой мы пытаемся уменьшить количество признаков, характеризующих объект. Обычно мы уменьшаем количество признаков до 2-3 для того, чтобы получить возможность визуализировать данные.\n",
    "\n",
    "<img src='img/dst3-ml1-4_6.jpg'>\n",
    "\n",
    "Для чего это нужно? \n",
    "\n",
    "Мы уже с вами не раз работали с данными, которые характеризуются большим количеством признаков: вспомните данные о ценах на дома в Мельбурне, о квартирах в Москве, о вакансиях на hh.ru или о качестве вин. \n",
    "\n",
    "Это вполне стандартная ситуация — целевой признак зависит от множества других признаков. Нам бы хотелось отобразить комплексную зависимость (от всех признаков одновременно) на графике. Но проблема в том, что для визуализации нам просто не хватит числовых осей!\n",
    "\n",
    "Представим, что у нас есть целевой признак цены и он зависит от 20 параметров. Мы хотим построить диаграмму рассеяния, которая показывает зависимость от всех признаков сразу. Если мы возьмём трёхмерную визуализацию, то целевой признак мы отложим по оси $z$, один из числовых признаков — по оси $x$, ещё один — по оси $y$. Оси закончились! Мы не можем отобразить четырёхмерную визуализацию.\n",
    "\n",
    "В модуле по визуализации мы говорили, что при большом желании можно также добавить признаки в виде цвета, формы и размеров точек на графике. Но, как бы мы ни старались, все 20 признаков на диаграмме нам не уместить. Кроме того, такая диаграмма становится всё менее читабельной с добавлением каждого нового признака. Вот пример диаграммы:\n",
    "\n",
    "<img src='img/dst3-ml1-4_7.jpg'>\n",
    "\n",
    "Выход из этой ситуации — использовать методы `понижения размерности`.\n",
    "\n",
    "Практическая польза этих методов состоит в том, что мы можем `объединить несколько признаков в один `и получить абстракцию.\n",
    "\n",
    "- Например, собаки с треугольными ушами, длинными носами и большими хвостами объединяются в полезную абстракцию «овчарки». Да, мы теряем информацию о конкретных овчарках, но новая абстракция точно полезнее этих лишних деталей.\n",
    "\n",
    "- Ещё одно проявление пользы таких алгоритмов — увеличение скорости обучения. Мы уже говорили о проклятии размерности: «чем больше признаков в данных, тем сложнее модели обучиться на них». Методы понижения размерности позволяют свести эту проблему к минимуму. Однако точность моделей может сильно упасть, поэтому необходимо уметь находить баланс.\n",
    "\n",
    "- Ещё один приятный бонус — мы автоматически избавляемся от мультиколлинеарности признаков. Методы понижения размерности устроены так, что в первую очередь объединяют между собой наиболее коррелированные признаки.\n",
    "\n",
    "> Примеры использования методов понижения размерности: визуализация, рекомендательные системы, определение тематик и поиск похожих между собой документов, анализ фейковых изображений.\n",
    "\n",
    "Инструмент на удивление хорошо подошёл для определения тематик текстов (Topic Modelling). Благодаря понижению размерности можно абстрагироваться от конкретных слов до уровня смыслов даже без привлечения учителя со списком категорий. Алгоритм назвали «латентно-семантический анализ» ([LSA](https://habr.com/post/110078/)), и его идея состоит в том, что частота появления слова в тексте зависит от его тематики: в научных статьях больше технических терминов, в новостях о политике — имён политиков.\n",
    "\n",
    "Другое популярное применение метода уменьшения размерности — рекомендательные системы. Оказалось, если обобщать оценки пользователей, получается неплохая система рекомендаций кино, музыки, игр и чего угодно вообще. У Яндекса есть хорошая [лекция](https://habr.com/ru/company/yandex/blog/241455/), посвящённая данной тематике. \n",
    "\n",
    "> **Цель обучения** — построить модель, которая переводит пространство признаков из размерности $n$ в размерность $m \\quad (n < m)$, при этом сохранив наибольший объём информации. Математически это записывается как $ a: X^n \\rightarrow X^m$.\n",
    "\n",
    "На рисунке ниже представлен пример понижения размерности данных с двух признаков до одного. Используемый алгоритм — `метод главных компонент (Principal Component Analysis, PCA)`. С помощью него находится обобщающая ось (компонента), которая содержит наибольшее количество информации о признаках. Наблюдения проецируются на новую ось, в результате чего получается новый признак, который является обобщением двух предыдущих. Обучение состоит в поиске оптимальной оси обобщения, которая содержит наибольший объём информации.\n",
    "\n",
    "<img src='img/dst3-ml1-4_8.jpg'>\n",
    "\n",
    "Основные алгоритмы понижения размерности:\n",
    "\n",
    "- метод главных компонент (PCA),\n",
    "- сингулярное разложение (SVD),\n",
    "- латентное размещение Дирихле (LDA),\n",
    "- латентный семантический анализ (LSA),\n",
    "- t-SNE.\n",
    "\n",
    "#### Ассоциация (бонус)\n",
    "\n",
    "<img src='img/dst3-ml1-4_9.jpg'>\n",
    "\n",
    "`Ассоциация` (association) — это задача, в которой мы пытаемся найти правила и законы, по которым существует последовательность действий. Давайте сразу перейдём к примерам.\n",
    "\n",
    "<img src='img/dst3-ml1-4_10.jpg'>\n",
    "\n",
    "Предположим, покупатель берёт в дальнем углу магазина мясо и идёт на кассу. Стоит ли ставить на его пути пакетики с сухой смесью для этого мяса? Часто ли люди берут их вместе? Мясо с приправами, вероятно, да. Но какие ещё товары покупают вместе? Когда вы владелец сети гипермаркетов, ответ для вас не всегда очевиден, но одно тактическое улучшение в расстановке товаров может принести хорошую прибыль.\n",
    "\n",
    "Мы можем создавать различные шаблоны, такие как определённые группы предметов, которые постоянно покупаются вместе, предметы, похожие на предметы, которые вы просматриваете, и т. д. Это, в свою очередь, помогает в правильном размещении предметов на веб-сайте или внутри физического заведения.\n",
    "\n",
    "Примеры использования: прогноз акций и распродаж, анализ товаров, покупаемых вместе, расстановка товаров на полках, анализ паттернов поведения на веб-сайтах.\n",
    "\n",
    "Основные ассоциативные модели:\n",
    "\n",
    "- Apriori,\n",
    "- Eclat,\n",
    "- FP Growth.\n",
    "\n",
    "На сегодняшний день данная область машинного обучения является наименее популярной и медленно развивающейся. Мы не будем детально рассматривать задачу ассоциации в нашем курсе, так как область её применения довольно специфичная и узконаправленная."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myConda_env_312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

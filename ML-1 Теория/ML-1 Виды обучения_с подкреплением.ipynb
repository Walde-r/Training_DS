{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "302b566d",
   "metadata": {},
   "source": [
    " Особенный вид машинного обучения — `обучение с подкреплением.\n",
    "\n",
    " Вспомним, где оно располагается на нашей карте:\n",
    "\n",
    " <img src='img/dst3-ml1-5_1.jpg'>\n",
    "\n",
    " #### ОБУЧЕНИЕ С ПОДКРЕПЛЕНИЕМ\n",
    "\n",
    " Обучение с подкреплением кардинально отличается от обучения с учителем и без него, поэтому его выделяют в отдельный вид обучения.\n",
    "\n",
    " > Это не задачи, связанные с анализом данных и предсказанием, а задачи взаимодействия со средой и «выживания» в ней. \n",
    "\n",
    " Средой может быть видеоигра: в такой искусственной среде «выживают», например, роботы, играющие в Mario, или интеллектуальные боты во множестве других игр.\n",
    "\n",
    " <img src='img/dst3-ml1-5_2.jpg'>\n",
    "\n",
    " Средой может быть и реальный мир, точнее — его часть: в такой среде существуют, например, автопилот Tesla, роботы-пылесосы или беспилотные летательные аппараты.\n",
    "\n",
    " Объект, который взаимодействует со средой (например, играет в игру), называется агентом.\n",
    "\n",
    " Агент может получать от среды полные или частичные наблюдения о её состоянии. Он может выполнять действия согласно своим наблюдениям. По мере совершения действий агент может получить в ответ награду от среды.\n",
    "\n",
    "<img src='img/dst3-ml1-5_3.jpg'>\n",
    "\n",
    "Например, в игре Mario действиями являются «прыгнуть», «сходить вперёд» или «присесть», а наградой — монетки, которые собирает персонаж.\n",
    "\n",
    "> Данные о среде могут быть полезны агенту, но они не являются главным фактором обучения. Неважно, сколько данных соберёт агент, — у него всё равно не получится предусмотреть все возможные ситуации.\n",
    "\n",
    "Поэтому цель обучения — не рассчитать все ходы, а построить оптимальную стратегию для взаимодействия со средой и максимизировать финальную награду.\n",
    "\n",
    "Выживание в среде — это и есть идея обучения с подкреплением. Давайте бросим бедного агента на растерзание судьбе и дадим ему неограниченное число жизней. Будем штрафовать его за ошибки и награждать за правильные поступки.\n",
    "\n",
    "Автомобили на автопилоте обучаются именно так: для них создают виртуальный город (часто на основе карт настоящих городов), населяют случайными пешеходами и отправляют учиться не сбивать всё подряд. Когда робот начинает хорошо себя чувствовать в искусственном городе, его выпускают тестировать на реальные улицы.\n",
    "\n",
    "Важно отметить, что в обучении с подкреплением машина не запоминает каждое движение, а пытается обобщить ситуации, чтобы выходить из них с максимальной выгодой. Но как она это делает? С помощью науки, конечно, имя которой — математика.\n",
    "\n",
    "В математике уже давно известны методы, которые позволяют находить оптимальную стратегию. В основе обучения с подкреплением лежат теория игр, теория динамической оптимизации и ещё множество математических дисциплин.\n",
    "\n",
    "Например, классический метод обучения с подкреплением — Q-learning — основан на [уравнении Беллмана](https://habr.com/ru/post/443240/).\n",
    "\n",
    "> В `Q-learning` рассматриваются все состояния, в которых может находиться агент, и все возможные переходы из одного состояния в другое, которые определяются действиями.\n",
    "\n",
    "Например, в игре Frozen Lake состояние определяется номером клетки на двумерном поле (на картинке их 16 штук). Наступив на некоторые клетки мы можем попасть в яму — это будет состояние проигрыша, а есть одна целевая клетка, попав в которую мы переходим в состояние выигрыша.\n",
    "\n",
    "Вводится некоторая Q-функция, которая оценивает, какую награду получит агент при совершении действия из своего состояния.\n",
    "\n",
    "> Уравнение Беллмана помогает определить следующее оптимальное действие, такое, что значение Q-функции для определённой пары состояние-действие будет максимальной.\n",
    "\n",
    "> `Цель Q-learning` — приближённо найти (аппроксимировать) Q-функцию, которая ответит на вопрос, как нужно правильно играть, чтобы получить максимум награды.\n",
    "\n",
    "Однако чем больше состояний, тем сложнее отыскать Q-функцию. Что если клеток на поле не 16, а 160? А если это не поле, разделённое на клетки, а целая игровая карта со множеством возможных ходов? Наконец, а если это улицы огромного города? Математически это будет означать экспоненциальное увеличение сложности поиска Q-функции.\n",
    "\n",
    "Для сложных задач — сложные решения. В таких случаях для поиска сложных Q-функций привлекаются нейронные сети, а такое обучение носит название `Deep Q-Network` (DQN).\n",
    "\n",
    "На идеях Q-learning основаны и другие алгоритмы, например алгоритм SARSA. Подробнее об алгоритмах можно почитать [здесь](https://habr.com/ru/post/561746/).\n",
    "\n",
    "Отдельный пласт в сфере обучения с подкреплением — генетические алгоритмы. Их идея отличается от идей Q-learning и состоит в следующем: мы бросаем множество агентов в среду и заставляем их идти к цели. Затем мы выбираем лучших из них — тех, кто прошёл дальше всех, скрещиваем, добавляем мутации и бросаем в среду ещё раз. Такие манипуляции мы проделываем огромное количество раз. В итоге по законам эволюции должно получиться разумное существо. Главный вопрос — сколько времени на это может понадобиться.\n",
    "\n",
    "> Хорошая визуализация работы генетических алгоритмов представлена [здесь](https://rednuht.org/genetic_walkers/)\n",
    "\n",
    "> Как выяснилось на практике, генетические алгоритмы значительно уступают в скорости обучения методам Q-learning и их производным, поэтому они используются всё реже.\n",
    "\n",
    "Мы разобрали особенности обучения с подкреплением. Очевидно, что оно не похоже на другие виды машинного обучения.\n",
    "\n",
    "Мы не будем детально рассматривать обучение с подкреплением в нашем курсе, так как область его применения, как вы сами понимаете, очень специфична, и вы сможете познакомиться с ней самостоятельно, если встретите её на практике.\n",
    "\n",
    "Однако важно понимать ключевые моменты в обучении с подкреплением, чтобы, если столкнётесь с описанием задачи, вы могли бы сразу определить её тип и правильно начать поиски решения.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myConda_env_312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
